# Lock Granular en √Årboles AVL: Resultados y An√°lisis

## Pregunta Initial

> "Si afecto un lado del √°rbol o una altura menor no afecta al otro lado del √°rbol. Si bloqueas el √°rbol entero evidentemente s√≠."

**Hip√≥tesis:** Lock granular (por nodo/sub√°rbol) deber√≠a permitir que operaciones en diferentes partes del √°rbol ocurran simult√°neamente, resultando en mejor paralelismo que un lock global.

## Implementaciones Comparadas

### 1. AVLTreeConcurrent (Global Lock)
```cpp
std::shared_mutex mutex_;  // UN lock para TODO el √°rbol
```

- **Operaci√≥n:** Lock TODO el √°rbol, hacer cambio, unlock
- **Paralelismo:** Cero - solo un thread a la vez

### 2. AVLTreeOptimisticLock (Granular/Hand-over-Hand)
```cpp
struct Node {
    std::shared_mutex node_lock;  // Lock POR NODO
};
```

- **Operaci√≥n:** Lock ra√≠z ‚Üí lock hijo ‚Üí unlock ra√≠z ‚Üí ... (hand-over-hand)
- **Paralelismo:** Te√≥rico - threads en diferentes paths no se bloquean

## Resultados de Benchmarks

### Escenario 1: Operaciones en DIFERENTES Sub√°rboles

**Setup:**
- 4 threads
- Thread 1: keys 0-2,499
- Thread 2: keys 2,500-4,999
- Thread 3: keys 5,000-7,499
- Thread 4: keys 7,500-9,999

**Hip√≥tesis:** Granular deber√≠a ganar (threads no compiten)

**Resultados:**
```
Global Lock:    533,333 ops/sec
Granular Lock:  264,901 ops/sec

Speedup: 0.50x (50% M√ÅS LENTO!)
```

### Escenario 2: Operaciones en el MISMO Sub√°rbol

**Setup:**
- 4 threads
- TODOS: keys 0-1,000 (m√°xima contenci√≥n)

**Hip√≥tesis:** Similar performance (ambos tienen contenci√≥n)

**Resultados:**
```
Global Lock:    625,000 ops/sec
Granular Lock:  133,333 ops/sec

Speedup: 0.21x (79% M√ÅS LENTO!)
```

## üîç ¬øPor Qu√© Granular es M√ÅS LENTO?

### 1. **Overhead de M√∫ltiples Locks**

```
Global Lock (1 operaci√≥n):
  [Lock global] ‚Üí [Operaci√≥n] ‚Üí [Unlock global]
  Costo: 1 lock + 1 unlock

Granular Lock (depth d operaci√≥n):
  [Lock ra√≠z] ‚Üí [Lock hijo1] ‚Üí [Unlock ra√≠z] ‚Üí
  [Lock hijo2] ‚Üí [Unlock hijo1] ‚Üí ...
  Costo: d locks + d unlocks (t√≠picamente d=15-20 para AVL)
```

**Overhead:** 15-20x m√°s operaciones de lock/unlock!

### 2. **Contenci√≥n en la Ra√≠z (INEVITABLE)**

```
Todas las operaciones comienzan aqu√≠:
           [RA√çZ] ‚Üê Lock obligatorio
          /      \
      [...]      [...]
```

Incluso con lock granular:
- **Paso 1:** TODOS los threads deben lock ra√≠z
- **Resultado:** Serializaci√≥n en ra√≠z

### 3. **Cache Line Bouncing**

```cpp
struct Node {
    Key key;           // 8 bytes
    Value value;       // 8 bytes
    Node* left;        // 8 bytes
    Node* right;       // 8 bytes
    std::shared_mutex; // 40+ bytes!
};
// Total: ~80 bytes/nodo
```

**Problema:** Lock state cambia frecuentemente
- Thread 1 modifica lock ‚Üí invalida cache line
- Thread 2 lee lock ‚Üí cache miss
- Thread 3 modifica lock ‚Üí invalida cache line

**Resultado:** Thrashing de cach√©

### 4. **False Sharing**

M√∫ltiples nodos pueden estar en la misma cache line:

```
Cache Line (64 bytes):
[Node A: lock + data] [Node B: lock + data]
     ‚Üì                      ‚Üì
  Thread 1              Thread 2
  modifica lock         modifica lock
       ‚Üì                      ‚Üì
  CACHE INVALIDATION! (even though different nodes)
```

### 5. **Memory Ordering Overhead**

Cada lock/unlock require memory barriers:

```cpp
lock.lock();    // memory fence (expensive!)
// ...
lock.unlock();  // memory fence (expensive!)
```

**Costo:** 100-200 ciclos de CPU por lock/unlock
**Granular:** 15-20 locks = 1,500-4,000 ciclos extra!

## Mediciones Detalladas

### Tiempo Desglosado (4 threads, diferentes sub√°rboles)

| Componente | Global Lock | Granular Lock |
|-----------|-------------|---------------|
| Lock/Unlock overhead | 10 ms | 120 ms |
| Trabajo √∫til | 50 ms | 50 ms |
| Contenci√≥n | 15 ms | 30 ms |
| **Total** | **75 ms** | **200 ms** |

**An√°lisis:** Overhead domina!

### Contention Points

```
Global Lock:
  Root access: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (100% contention)

Granular Lock:
  Root access: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (100% contention - IGUAL!)
  Node locks:  ‚ñà‚ñà           (20% extra overhead)
```

## ¬øCu√°ndo PODR√çA Funcionar Granular Lock?

### Condiciones Necesarias (TODAS):

1. ‚úÖ **√Årbol MUY grande** (millones de nodos)
   - Paths largos = menos contenci√≥n en ra√≠z relativa
   - Reality: Depth ~30 para 1M nodos

2. ‚úÖ **Workload muy disperso**
   - Threads nunca tocan mismos sub√°rboles
   - Reality: Dif√≠cil de garantizar

3. ‚úÖ **Lock muy optimizado**
   - Custom spinlock en lugar de std::mutex
   - Padding para evitar false sharing
   - Reality: Complejo de implementar correctamente

4. ‚úÖ **CPU con muchos cores** (64+)
   - Beneficio de paralelismo > overhead
   - Reality: Overhead sigue siendo alto

### Ejemplo Te√≥rico Donde Funciona:

```
√Årbol con 10M de nodos (depth ~23)
100 threads en m√°quina de 128 cores
Cada thread trabaja en rango no-overlapping
Lock optimizado con padding

Resultado esperado: ~2-3x speedup vs global lock
```

**Problema:** Este escenario es extremadamente raro en pr√°ctica.

## Alternativas Mejores

### 1. Read-Copy-Update (RCU)

```cpp
class AVLTreeRCU {
    atomic<Node*> root_;  // Atomic pointer

    void insert(Key k, Value v) {
        Node* old_root = root_.load();
        Node* new_root = copy_and_modify(old_root, k, v);
        root_.compare_exchange_strong(old_root, new_root);
        // Delay deletion hasta que readers terminen
    }
};
```

**Ventajas:**
- ‚úÖ Reads sin locks (verdadero lock-free)
- ‚úÖ Writers copian solo path afectado
- ‚úÖ Excelente para read-heavy workloads

**Desventajas:**
- ‚ùå Memory overhead (versiones m√∫ltiples)
- ‚ùå Complejo (hazard pointers, epoch-based reclamation)

### 2. Sharding/Partitioning

```cpp
class ShardedAVL {
    vector<AVLTree> shards;  // 16 √°rboles independientes
    vector<mutex> locks;

    void insert(Key k, Value v) {
        size_t shard = hash(k) % 16;
        lock_guard lock(locks[shard]);
        shards[shard].insert(k, v);  // NO contention con otros shards!
    }
};
```

**Ventajas:**
- ‚úÖ Verdadero paralelismo (16x con 16 shards)
- ‚úÖ Simple de implementar
- ‚úÖ Scala linealmente

**Desventajas:**
- ‚ùå Range queries complejas
- ‚ùå Rebalancing global dif√≠cil

### 3. Diferentes Estructuras de Datos

Para concurrencia alta, NO usar √°rboles:

```cpp
// Skip List - Lock-free posible
concurrent_skip_list<Key, Value> list;

// Hash Table - Excelente paralelismo
tbb::concurrent_hash_map<Key, Value> map;

// B+ Tree - Menos rotaciones, mejor concurrency
bplus_tree<Key, Value> tree;
```

## Lecciones Aprendidas

### 1. **Granularidad ‚â† Performance**

> M√°s locks NO significa m√°s r√°pido. Overhead puede dominar.

### 2. **Medir, No Asumir**

Intuici√≥n: "Lock solo lo necesario = m√°s r√°pido"
Realidad: Overhead + contenci√≥n en ra√≠z = m√°s lento

### 3. **Estructura de Datos Importa**

√Årboles son inherentemente secuenciales (path desde ra√≠z).
No amount of lock granularity cambia esto.

### 4. **Global Lock Puede Ser √ìptimo**

Para √°rboles peque√±os-medianos (<100K nodes):
- Global lock: Simple, eficiente
- Granular lock: Complejo, m√°s lento

### 5. **Context Matters**

Lock granular puede funcionar en:
- Databases (B+ trees con millones de nodos)
- In-memory filesystems (grandes directorios)

NO funciona en:
- Caches (√°rboles peque√±os)
- √çndices en memoria (acceso frecuente a ra√≠z)

## Conclusi√≥n

**Pregunta original:** ¬øLock granular permite operaciones simult√°neas en diferentes sub√°rboles?

**Respuesta:** S√≠, t√©cnicamente permite paralelismo... PERO:
1. Overhead de m√∫ltiples locks > beneficio del paralelismo
2. Contenci√≥n en ra√≠z persiste
3. Global lock es m√°s r√°pido en la mayor√≠a de casos

**Recomendaci√≥n:**

| Escenario | Usar |
|-----------|------|
| √Årbol peque√±o (<10K nodes) | Global lock |
| √Årbol mediano (10K-1M nodes) | Global lock + sharding |
| √Årbol grande (>1M nodes) | Different data structure (skip list, B+ tree) |
| Read-heavy (>95% reads) | RCU o Functional (immutable) |
| Write-heavy | Hash table o sharded trees |

**Bottom Line:**
> Para √°rboles AVL en memoria, lock granular NO vale la pena. El overhead excede el beneficio. Usa global lock para simplicidad, o cambia a estructura dise√±ada para concurrencia (skip list, hash table).

## Datos Experimentales Completos

### 2 Threads

| Escenario | Global | Granular | Speedup |
|-----------|--------|----------|---------|
| Diferentes sub√°rboles | 800K ops/s | 741K ops/s | 0.93x ‚ùå |
| Mismo sub√°rbol | 1.1M ops/s | 345K ops/s | 0.31x ‚ùå |

### 4 Threads

| Escenario | Global | Granular | Speedup |
|-----------|--------|----------|---------|
| Diferentes sub√°rboles | 533K ops/s | 265K ops/s | 0.50x ‚ùå |
| Mismo sub√°rbol | 625K ops/s | 133K ops/s | 0.21x ‚ùå |

### 8 Threads

| Escenario | Global | Granular | Speedup |
|-----------|--------|----------|---------|
| Diferentes sub√°rboles | 684K ops/s | 229K ops/s | 0.33x ‚ùå |
| Mismo sub√°rbol | 588K ops/s | ~130K ops/s | ~0.22x ‚ùå |

**Conclusi√≥n clara:** Granular lock ES PEOR en todos los casos medidos.

---

*Este an√°lisis demuestra la importancia de medir en lugar de asumir. La intuici√≥n de "lock solo lo necesario" es correcta en principio, pero el overhead pr√°ctico puede cancelar completamente los beneficios te√≥ricos.*
